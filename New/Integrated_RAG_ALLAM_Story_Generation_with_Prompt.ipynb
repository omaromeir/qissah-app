{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a1931fd",
   "metadata": {},
   "source": [
    "# Integrated RAG-based Story Generation with IBM's ALLAM and ChromaDB\n",
    "\n",
    "This notebook combines data embedding, ChromaDB storage, and IBM's ALLAM model for retrieval-augmented generation (RAG). It organizes the process as follows:\n",
    "- Embedding children's stories and storing them in ChromaDB for efficient retrieval.\n",
    "- Using IBM's ALLAM model to generate stories enhanced by retrieved, relevant story chunks or through prompting.\n",
    "\n",
    "## Steps\n",
    "1. Install libraries.\n",
    "2. Embed and store data in ChromaDB.\n",
    "3. Retrieve and store stories to ChromaDB.\n",
    "4. Setup IBM ALLAM model.\n",
    "5. RAG-based story generation or Prompt Engineering generation.\n",
    "6. Testing Functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c501d4",
   "metadata": {},
   "source": [
    "### Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbce181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries for ChromaDB, Firebase, IBM's ALLAM, and LangChain\n",
    "!pip install langchain chromadb requests ibm-watsonx-ai firebase-admin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a7e3e-e625-419e-a5a2-cac74fb0b932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain import PromptTemplate\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from chromadb import Client\n",
    "from firebase_admin import credentials, firestore\n",
    "from chromadb.config import Settings\n",
    "\n",
    "import os\n",
    "import json\n",
    "import firebase_admin\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3085ec",
   "metadata": {},
   "source": [
    "### Step 2: Data Embedding and Storage in ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9606f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize Firebase\n",
    "cred = credentials.Certificate('path/to/firebase_credentials.json')  # Adjust with actual path\n",
    "firebase_admin.initialize_app(cred)\n",
    "db = firestore.client()\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "chroma_client = Client(Settings())\n",
    "collection = chroma_client.get_or_create_collection(name='story_chunks')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6ef9bb-634b-4e5f-bb43-1b4ffbce9fea",
   "metadata": {},
   "source": [
    "### Step 3: Retrieve and store stories to ChromaDB\n",
    "- Retrieve stories from Firebase.\n",
    "- Process, chunk, and embed each story segment.\n",
    "- Store the chunks along with embeddings and metadata in ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862912ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_store_stories():\n",
    "    # Retrieve stories from Firebase, chunk them, and store in ChromaDB\n",
    "    documents, embeddings, metadata, ids = [], [], [], []\n",
    "    stories_ref = db.collection('stories')\n",
    "    stories = stories_ref.stream()\n",
    "\n",
    "    for idx, story_doc in enumerate(stories, start=1):\n",
    "        story_data = story_doc.to_dict().get('data', [])\n",
    "        metadata = {}\n",
    "        story_text = ''\n",
    "\n",
    "        # Process each item in the story data\n",
    "        for item in story_data:\n",
    "            if item.get('prompt') == 'القصة:':\n",
    "                story_text = item.get('completion', '')\n",
    "            else:\n",
    "                metadata[item.get('prompt', '')] = item.get('completion', '')\n",
    "\n",
    "        if story_text:\n",
    "            chunks = re.findall(r'(<[^>]+>.*?</[^>]+>)', story_text, flags=re.DOTALL)\n",
    "            for chunk_idx, chunk in enumerate(chunks):\n",
    "                # Generate embedding for each chunk\n",
    "                embedding = model.encode(chunk)\n",
    "                documents.append(chunk)\n",
    "                embeddings.append(embedding)\n",
    "                ids.append(f'id_{idx}_{chunk_idx}')\n",
    "                metadata.append({**metadata, 'story_id': idx, 'chunk_id': f'id_{idx}_{chunk_idx}'})\n",
    "\n",
    "    # Store in ChromaDB\n",
    "    collection.add(documents=documents, embeddings=embeddings, metadatas=metadata, ids=ids)\n",
    "    print('Data stored in ChromaDB.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e9fc8",
   "metadata": {},
   "source": [
    "### Step 4: Initialize IBM ALLAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c312854",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_credentials():\n",
    "    # Function to retrieve API credentials\n",
    "    return {\n",
    "        'url': 'https://eu-de.ml.cloud.ibm.com',\n",
    "        'apikey': getpass.getpass(\"n_mMMf-M68fePv4tTMw5OgipRtv4tB1HLdMeOaf16GRL\")\n",
    "    }\n",
    "\n",
    "# Define model parameters\n",
    "model_id = \"sdaia/allam-1-13b-instruct\"  # Defining model ID\n",
    "project_id = \"07954e26-b0dd-45e4-a22f-0ae8e0a8593a\"  # Defining project ID\n",
    "parameters = {'decoding_method': 'greedy', 'max_new_tokens': 3024, 'repetition_penalty': 1.2}\n",
    "\n",
    "# Instantiate the ALLAM model\n",
    "model = ModelInference(\n",
    "    model_id=model_id,\n",
    "    params=parameters,\n",
    "    credentials=get_credentials(),\n",
    "    project_id=project_id\n",
    ")\n",
    "print('ALLAM model initialized.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac91e5-0646-4698-8910-91488be66323",
   "metadata": {},
   "source": [
    "### Step 5: RAG-based Story Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30171345-2d84-42a9-9425-52607cb15fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_story_with_retrieval(characterType, characterCount, characters, storyLocation, storyMoral, otherThings):\n",
    "    \"\"\"\n",
    "    Retrieves relevant story chunks from ChromaDB, formats a structured prompt, \n",
    "    and generates a story using IBM's ALLAM model.\n",
    "    \"\"\"\n",
    "    # Construct retrieval prompt based on child’s input details\n",
    "    retrieval_prompt = (\n",
    "        f\"A story with {characterType}, set in {storyLocation}, focusing on the theme of {storyMoral} and {otherThings}.\"\n",
    "    )\n",
    "    query_embedding = allam_model.embed_text(retrieval_prompt)\n",
    "\n",
    "    # Retrieve relevant story chunks from ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=3\n",
    "    )\n",
    "    retrieved_content = \" \".join([doc for doc in results[\"documents\"][0]])\n",
    "\n",
    "    # Define the structured prompt template\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"retrieved_content\", \"characterType\", \"characterCount\", \"characters\", \"storyLocation\", \"storyMoral\", \"otherThings\"],\n",
    "        template=\"\"\"\n",
    "        Based on the following input details:\n",
    "        - Character Type: {characterType}\n",
    "        - Number of characters: {characterCount}\n",
    "        - Name of Characters: {characters}\n",
    "        - Setting: {storyLocation}\n",
    "        - Main Theme: {storyMoral}\n",
    "        - More details: {otherThings}\n",
    "        - Retrieved Content: {retrieved_content}\n",
    "\n",
    "        Use the details provided to write a short Arabic story for children aged 8-11, emphasizing Arabic culture. \n",
    "        Ensure it has a clear beginning, middle, and end, and conclude with a unique ending type.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Format the final prompt using PromptTemplate\n",
    "        formatted_prompt = prompt_template.format(\n",
    "        retrieved_content=retrieved_content,\n",
    "        characterType = characterType,\n",
    "        characterCount = characterCount,\n",
    "        characters = characters,\n",
    "        storyLocation = storyLocation,\n",
    "        storyMoral = storyMoral,\n",
    "        otherThings = otherThings\n",
    "    )\n",
    "\n",
    "    # Generate story using IBM ALLAM model\n",
    "    print(\"Submitting generation request to IBM ALLAM...\")\n",
    "    story_response = allam_model.generate(formatted_prompt)\n",
    "    return story_response['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3546d0b7",
   "metadata": {},
   "source": [
    "### or Step 5: Prompt Engineering Story Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52f5e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_story_with_prompt_engineering(characterType, characterCount, characters, storyLocation, storyMoral, otherThings):\n",
    "    \"\"\"\n",
    "    Generates a story using prompt engineering based on user input.\n",
    "    This function formats a structured prompt and sends it to IBM's ALLAM model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the prompt template based on input details\n",
    "    prompt = f\"\"\"\n",
    "    Write a short Arabic children's story suitable for ages 8-11 with the following details:\n",
    "    \n",
    "    - **Character Type**: {characterType}\n",
    "    - **Number of Characters**: {characterCount}\n",
    "    - **Character Names**: {characters}\n",
    "    - **Setting**: {storyLocation}\n",
    "    - **Main Theme or Moral**: {story_moral}\n",
    "    - **Additional Elements**: {other_things}\n",
    "    \n",
    "    Make sure the story has a clear beginning, middle, and end. Emphasize Arabic culture in a simple and age-appropriate way.\n",
    "    Conclude the story with a unique ending type, such as a happy ending, a surprise, or a moral lesson.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Send the prompt to IBM's ALLAM model\n",
    "    print(\"Submitting generation request to IBM ALLAM...\")\n",
    "    story_response = allam_model.generate(prompt)\n",
    "    \n",
    "    # Extract and return the generated story text\n",
    "    return story_response['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8258bc",
   "metadata": {},
   "source": [
    "## Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ace88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the generate_story_with RAG\n",
    "test_story = generate_story_with_retrieval(\n",
    "    character_type=\"animals and humans\",\n",
    "    character_count=\"3\",\n",
    "    characters=\"Ali, Lina, and a talking owl\",\n",
    "    story_location=\"in a village and forest\",\n",
    "    story_moral=\"importance of honesty\",\n",
    "    other_things=\"magical events and a surprise ending\"\n",
    ")\n",
    "\n",
    "print(\"Generated Story:\", test_story)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980471b3-0a30-42dd-a90d-983bb83ee66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the generate_story_with prompt engineering\n",
    "test_story = generate_story_with_prompt_engineering(\n",
    "    character_type=\"animals and humans\",\n",
    "    character_count=\"3\",\n",
    "    characters=\"Ali, Lina, and a talking owl\",\n",
    "    story_location=\"in a village and forest\",\n",
    "    story_moral=\"importance of honesty\",\n",
    "    other_things=\"magical events and a surprise ending\"\n",
    ")\n",
    "\n",
    "print(\"Generated Story:\", test_story)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fewshot",
   "language": "python",
   "name": "fewshot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
